{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgan_gp_curve_discriminator.py\n",
    "# PyTorch 1D WGAN-GP for curves + using the discriminator as a realism classifier\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 0) Utils: seeds & device\n",
    "# -----------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 1) (Demo) Synthetic curves: mixtures of sinusoids\n",
    "#    Replace this block with your real dataset loading.\n",
    "# -----------------------------------------\n",
    "def synth_curves(n, T=256, noise_std=0.05):\n",
    "    \"\"\"\n",
    "    Create 'real' curves: sum of 2-3 sinusoids with smooth envelopes.\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    t = np.linspace(0, 1, T)\n",
    "    for _ in range(n):\n",
    "        k = np.random.choice([2, 3])\n",
    "        y = np.zeros_like(t)\n",
    "        for _ in range(k):\n",
    "            amp = np.random.uniform(0.5, 1.5)\n",
    "            freq = np.random.uniform(1.0, 6.0)\n",
    "            phase = np.random.uniform(0, 2*np.pi)\n",
    "            env = 0.6 + 0.4*np.cos(2*np.pi*np.random.uniform(0.5, 1.5)*t + np.random.uniform(0, 2*np.pi))\n",
    "            y += amp * np.sin(2*np.pi*freq*t + phase) * env\n",
    "        y += np.random.normal(0, noise_std, size=T)\n",
    "        xs.append(y.astype(np.float32))\n",
    "    X = np.stack(xs, axis=0)  # (n, T)\n",
    "    return X\n",
    "\n",
    "def synth_ood_curves(n, T=256, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Create 'OOD' curves: spikes/square-ish waves to look different.\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    t = np.linspace(0, 1, T)\n",
    "    for _ in range(n):\n",
    "        # square-ish + spikes\n",
    "        base = np.sign(np.sin(2*np.pi*np.random.uniform(2, 5)*t + np.random.uniform(0, 2*np.pi)))\n",
    "        # random spikes\n",
    "        for _ in range(np.random.randint(2, 6)):\n",
    "            idx = np.random.randint(0, T)\n",
    "            width = np.random.randint(2, 8)\n",
    "            base[idx:idx+width] += np.random.uniform(2.0, 4.0)\n",
    "        base += np.random.normal(0, noise_std, size=T)\n",
    "        xs.append(base.astype(np.float32))\n",
    "    X = np.stack(xs, axis=0)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 2) Dataset & normalization\n",
    "# -----------------------------------------\n",
    "class CurveDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, mean: Optional[np.ndarray]=None, std: Optional[np.ndarray]=None):\n",
    "        \"\"\"\n",
    "        X shape: (N, T). We store as (N, 1, T) for Conv1d.\n",
    "        If mean/std are given, apply them; else compute per-timepoint stats from X.\n",
    "        \"\"\"\n",
    "        assert X.ndim == 2\n",
    "        self.X = X.copy().astype(np.float32)\n",
    "        self.N, self.T = self.X.shape\n",
    "        if mean is None or std is None:\n",
    "            self.mean = self.X.mean(axis=0, keepdims=True)\n",
    "            self.std = self.X.std(axis=0, keepdims=True) + 1e-6\n",
    "        else:\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "        self.X = self.X[:, None, :]  # (N, 1, T)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        return torch.from_numpy(x)\n",
    "\n",
    "\n",
    "def compute_norm_stats(X: np.ndarray):\n",
    "    mean = X.mean(axis=0, keepdims=True)\n",
    "    std = X.std(axis=0, keepdims=True) + 1e-6\n",
    "    return mean.astype(np.float32), std.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 3) Models: 1D Generator & Critic\n",
    "# -----------------------------------------\n",
    "class Generator1D(nn.Module):\n",
    "    def __init__(self, z_dim=64, T=256, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Project noise to a low-res temporal map, then upsample with ConvTranspose1d\n",
    "        self.init_T = T // 16  # 16x upsampling total\n",
    "        self.fc = nn.Linear(z_dim, base_ch * self.init_T)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base_ch, base_ch, kernel_size=4, stride=2, padding=1),  # x2\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.ConvTranspose1d(base_ch, base_ch//2, kernel_size=4, stride=2, padding=1),  # x4\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.ConvTranspose1d(base_ch//2, base_ch//4, kernel_size=4, stride=2, padding=1),  # x8\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.ConvTranspose1d(base_ch//4, base_ch//8, kernel_size=4, stride=2, padding=1),  # x16 -> T\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv1d(base_ch//8, 1, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)  # (B, base_ch * init_T)\n",
    "        B = x.shape[0]\n",
    "        # reshape to (B, C, T0)\n",
    "        C = x.shape[1] // self.init_T\n",
    "        x = x.view(B, C, self.init_T)\n",
    "        x = self.net(x)\n",
    "        return x  # (B, 1, T)\n",
    "\n",
    "\n",
    "class Critic1D(nn.Module):\n",
    "    def __init__(self, T=256, base_ch=64, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, base_ch, kernel_size=7, padding=3),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv1d(base_ch, base_ch, kernel_size=5, stride=2, padding=2),  # /2\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv1d(base_ch, base_ch*2, kernel_size=5, stride=2, padding=2),  # /4\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv1d(base_ch*2, base_ch*4, kernel_size=5, stride=2, padding=2),  # /8\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv1d(base_ch*4, base_ch*4, kernel_size=5, stride=2, padding=2),  # /16\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        # compute feature length after strides\n",
    "        feat_T = T // 16\n",
    "        self.flat = nn.Linear(base_ch*4*feat_T, embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, 1)  # Wasserstein score\n",
    "\n",
    "    def forward(self, x, return_embed=False):\n",
    "        h = self.features(x)\n",
    "        h = h.flatten(1)\n",
    "        e = self.flat(h)\n",
    "        s = self.out(F.leaky_relu(e, 0.2))\n",
    "        if return_embed:\n",
    "            return s.squeeze(1), e\n",
    "        return s.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 4) WGAN-GP training step helpers\n",
    "# -----------------------------------------\n",
    "def gradient_penalty(critic, real, fake):\n",
    "    B = real.size(0)\n",
    "    eps = torch.rand(B, 1, 1, device=real.device)\n",
    "    inter = eps * real + (1 - eps) * fake\n",
    "    inter.requires_grad_(True)\n",
    "    scores = critic(inter)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=scores,\n",
    "        inputs=inter,\n",
    "        grad_outputs=torch.ones_like(scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    grads = grads.view(B, -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 5) Training\n",
    "# -----------------------------------------\n",
    "def train_wgan_gp(\n",
    "    train_loader,\n",
    "    T=256,\n",
    "    z_dim=64,\n",
    "    g_lr=1e-4,\n",
    "    d_lr=1e-4,\n",
    "    lambda_gp=10.0,\n",
    "    n_epochs=200,\n",
    "    n_critic=5,\n",
    "    print_every=50\n",
    "):\n",
    "    G = Generator1D(z_dim=z_dim, T=T).to(device)\n",
    "    D = Critic1D(T=T).to(device)\n",
    "\n",
    "    g_opt = torch.optim.Adam(G.parameters(), lr=g_lr, betas=(0.5, 0.9))\n",
    "    d_opt = torch.optim.Adam(D.parameters(), lr=d_lr, betas=(0.5, 0.9))\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        for i, real in enumerate(train_loader):\n",
    "            real = real.to(device)  # (B, 1, T)\n",
    "\n",
    "            # 1) Update Critic n_critic times\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(real.size(0), z_dim, device=device)\n",
    "                fake = G(z).detach()\n",
    "                d_real = D(real)\n",
    "                d_fake = D(fake)\n",
    "                gp = gradient_penalty(D, real, fake)\n",
    "\n",
    "                d_loss = -(d_real.mean() - d_fake.mean()) + lambda_gp * gp\n",
    "\n",
    "                d_opt.zero_grad(set_to_none=True)\n",
    "                d_loss.backward()\n",
    "                d_opt.step()\n",
    "\n",
    "            # 2) Update Generator once\n",
    "            z = torch.randn(real.size(0), z_dim, device=device)\n",
    "            fake = G(z)\n",
    "            g_loss = -D(fake).mean()\n",
    "\n",
    "            g_opt.zero_grad(set_to_none=True)\n",
    "            g_loss.backward()\n",
    "            g_opt.step()\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch:04d}] D_loss={d_loss.item():.4f} | G_loss={g_loss.item():.4f}\")\n",
    "\n",
    "    return G, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 6) Scoring, thresholding, evaluation\n",
    "# -----------------------------------------\n",
    "@torch.no_grad()\n",
    "def critic_scores(D, loader):\n",
    "    scores = []\n",
    "    for x in loader:\n",
    "        x = x.to(device)\n",
    "        s = D(x)  # (B,)\n",
    "        scores.append(s.cpu().numpy())\n",
    "    return np.concatenate(scores, axis=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def critic_embeddings(D, loader):\n",
    "    embs = []\n",
    "    for x in loader:\n",
    "        x = x.to(device)\n",
    "        _, e = D(x, return_embed=True)\n",
    "        embs.append(e.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "def choose_threshold_from_id(scores_id, fpr_target=0.05):\n",
    "    # lower tail is \"less real\" for WGAN critics if trained as above\n",
    "    q = np.quantile(scores_id, fpr_target)\n",
    "    return q\n",
    "\n",
    "def auroc(y_true, y_score):\n",
    "    # simple AUROC (Mannâ€“Whitney U)\n",
    "    y_true = np.array(y_true)\n",
    "    y_score = np.array(y_score)\n",
    "    pos = y_score[y_true == 1]\n",
    "    neg = y_score[y_true == 0]\n",
    "    # P(score_pos > score_neg) + 0.5*P(=)\n",
    "    total = 0\n",
    "    ties = 0\n",
    "    for p in pos:\n",
    "        total += np.sum(p > neg) + 0.5*np.sum(p == neg)\n",
    "    return total / (len(pos)*len(neg) + 1e-12)\n",
    "\n",
    "def train_test_split(X, ratio_train=0.8):\n",
    "    N = X.shape[0]\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    n_train = int(N * ratio_train)\n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:]\n",
    "    return X[train_idx], X[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 7) Mahalanobis score on critic embeddings (optional)\n",
    "# -----------------------------------------\n",
    "class GaussianEmbed:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.cov_inv = None\n",
    "\n",
    "    def fit(self, E):  # E: (N, d)\n",
    "        self.mean = E.mean(axis=0, keepdims=True)\n",
    "        cov = np.cov(E.T) + 1e-5*np.eye(E.shape[1])\n",
    "        self.cov_inv = np.linalg.inv(cov)\n",
    "\n",
    "    def maha(self, E):  # smaller is more ID\n",
    "        diff = E - self.mean\n",
    "        return np.einsum('nd,dk,nk->n', diff, self.cov_inv, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0001] D_loss=-12.0021 | G_loss=2.1986\n",
      "[Epoch 0050] D_loss=-1.1060 | G_loss=3.0608\n",
      "[Epoch 0100] D_loss=-1.1692 | G_loss=1.5935\n",
      "[Epoch 0150] D_loss=-1.0049 | G_loss=2.5962\n",
      "[Epoch 0200] D_loss=-1.6637 | G_loss=1.7923\n",
      "Chosen threshold tau (5% ID FPR): -7.119\n",
      "Chosen threshold tau_emb (5% ID FPR in embed): -306.201\n",
      "Saved checkpoints/skin_wgan_gp.pt\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 8) Glue: Putting it all together\n",
    "# -----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Config ---\n",
    "    T = 128\n",
    "    ratio_train = 0.8\n",
    "    batch_size = 128\n",
    "    z_dim = 32\n",
    "\n",
    "    # --- Data (replace with your arrays of shape (N, T)) ---\n",
    "    skin_reflectance = pd.read_csv('../data/resample_skin_reflectance.csv').to_numpy()\n",
    "    derivative = pd.read_csv('../data/resample_derivative.csv').to_numpy()\n",
    "    X_id = np.concatenate((skin_reflectance, derivative), axis=1)\n",
    "\n",
    "    X_train_id, X_val_id = train_test_split(X_id, ratio_train=ratio_train)\n",
    "    \n",
    "\n",
    "    # --- Normalize using training stats only ---\n",
    "    mean, std = compute_norm_stats(X_train_id)\n",
    "    train_ds = CurveDataset(X_train_id, mean, std)\n",
    "    val_id_ds = CurveDataset(X_val_id, mean, std)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_id_loader = DataLoader(val_id_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # --- Train WGAN-GP ---\n",
    "    G, D = train_wgan_gp(\n",
    "        train_loader,\n",
    "        T=T,\n",
    "        z_dim=z_dim,\n",
    "        g_lr=1e-4,\n",
    "        d_lr=1e-4,\n",
    "        lambda_gp=10.0,\n",
    "        n_epochs=200,       # for real data you may go longer; tune as needed\n",
    "        n_critic=5,\n",
    "        print_every=50\n",
    "    )\n",
    "\n",
    "    # --- Freeze critic and score validation splits ---\n",
    "    D.eval()\n",
    "    s_val_id = critic_scores(D, val_id_loader)\n",
    "\n",
    "    # --- Threshold selection ---\n",
    "    # Option A: target FPR on ID only\n",
    "    tau = choose_threshold_from_id(s_val_id, fpr_target=0.05)\n",
    "\n",
    "    print(f\"Chosen threshold tau (5% ID FPR): {tau:.3f}\")\n",
    "\n",
    "    # Option B (if OOD labels are available): simple AUROC\n",
    "\n",
    "    # --- Test evaluation ---\n",
    "\n",
    "    # Predictions based on tau\n",
    "\n",
    "    # --- Optional: Mahalanobis on embeddings ---\n",
    "    E_train_id = critic_embeddings(D, train_loader)\n",
    "    E_val_id = critic_embeddings(D, val_id_loader)\n",
    "    ge = GaussianEmbed()\n",
    "    ge.fit(E_train_id)\n",
    "    m_val_id = -ge.maha(E_val_id)  # higher is more ID, so take negative distance\n",
    "    tau_emb = choose_threshold_from_id(m_val_id, fpr_target=0.05)\n",
    "    print(f\"Chosen threshold tau_emb (5% ID FPR in embed): {tau_emb:.3f}\")\n",
    "    \n",
    "\n",
    "    # --- Save for later inference ---\n",
    "    ckpt = {\n",
    "        \"G\": G.state_dict(),\n",
    "        \"D\": D.state_dict(),\n",
    "        \"ge\": ge,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"tau\": tau,\n",
    "        \"tau_emb\": tau_emb\n",
    "    }\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    torch.save(ckpt, \"checkpoints/skin_wgan_gp.pt\")\n",
    "    print(\"Saved checkpoints/skin_wgan_gp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hjh\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:179.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0001] D_loss=-18.6135 | G_loss=0.1249\n",
      "[Epoch 0050] D_loss=-3.0907 | G_loss=-16.0091\n",
      "[Epoch 0100] D_loss=-2.5250 | G_loss=-4.0485\n",
      "[Epoch 0150] D_loss=-2.1766 | G_loss=-3.3152\n",
      "[Epoch 0200] D_loss=-2.0812 | G_loss=0.9161\n",
      "[Epoch 0250] D_loss=-2.2007 | G_loss=0.7205\n",
      "[Epoch 0300] D_loss=-2.7123 | G_loss=4.5101\n",
      "[Epoch 0350] D_loss=-2.8464 | G_loss=-0.4621\n",
      "[Epoch 0400] D_loss=-2.3055 | G_loss=0.5632\n",
      "Validation AUROC (critic score): 0.955\n",
      "Chosen threshold tau (5% ID FPR): -1.578\n",
      "Test AUROC (critic score): 0.948\n",
      "Test accuracy @tau: 0.879\n",
      "Validation AUROC (Mahalanobis in critic embed): 1.000\n",
      "Test AUROC (Mahalanobis): 1.000\n",
      "Saved checkpoints/curve_wgan_gp.pt\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 8) Glue: Putting it all together\n",
    "# -----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Config ---\n",
    "    T = 256\n",
    "    N_train = 4000\n",
    "    N_val = 800\n",
    "    N_test = 800\n",
    "    batch_size = 128\n",
    "    z_dim = 64\n",
    "\n",
    "    # --- Data (replace with your arrays of shape (N, T)) ---\n",
    "    X_train_id = synth_curves(N_train, T=T)\n",
    "    X_val_id = synth_curves(N_val, T=T)\n",
    "    X_test_id = synth_curves(N_test, T=T)\n",
    "\n",
    "    # OOD splits for evaluation (if you don't have them, skip and use ID quantiles only)\n",
    "    X_val_ood = synth_ood_curves(N_val, T=T)\n",
    "    X_test_ood = synth_ood_curves(N_test, T=T)\n",
    "\n",
    "    # --- Normalize using training stats only ---\n",
    "    mean, std = compute_norm_stats(X_train_id)\n",
    "    train_ds = CurveDataset(X_train_id, mean, std)\n",
    "    val_id_ds = CurveDataset(X_val_id, mean, std)\n",
    "    test_id_ds = CurveDataset(X_test_id, mean, std)\n",
    "    val_ood_ds = CurveDataset(X_val_ood, mean, std)\n",
    "    test_ood_ds = CurveDataset(X_test_ood, mean, std)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_id_loader = DataLoader(val_id_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_id_loader = DataLoader(test_id_ds, batch_size=batch_size, shuffle=False)\n",
    "    val_ood_loader = DataLoader(val_ood_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_ood_loader = DataLoader(test_ood_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # --- Train WGAN-GP ---\n",
    "    G, D = train_wgan_gp(\n",
    "        train_loader,\n",
    "        T=T,\n",
    "        z_dim=z_dim,\n",
    "        g_lr=1e-4,\n",
    "        d_lr=1e-4,\n",
    "        lambda_gp=10.0,\n",
    "        n_epochs=400,       # for real data you may go longer; tune as needed\n",
    "        n_critic=5,\n",
    "        print_every=50\n",
    "    )\n",
    "\n",
    "    # --- Freeze critic and score validation splits ---\n",
    "    D.eval()\n",
    "    s_val_id = critic_scores(D, val_id_loader)\n",
    "    s_val_ood = critic_scores(D, val_ood_loader)\n",
    "\n",
    "    # --- Threshold selection ---\n",
    "    # Option A: target FPR on ID only\n",
    "    tau = choose_threshold_from_id(s_val_id, fpr_target=0.05)\n",
    "\n",
    "    # Option B (if OOD labels are available): simple AUROC\n",
    "    y_val = np.concatenate([np.ones_like(s_val_id), np.zeros_like(s_val_ood)])\n",
    "    s_val = np.concatenate([s_val_id, s_val_ood])\n",
    "    roc = auroc(y_val, s_val)\n",
    "    print(f\"Validation AUROC (critic score): {roc:.3f}\")\n",
    "    print(f\"Chosen threshold tau (5% ID FPR): {tau:.3f}\")\n",
    "\n",
    "    # --- Test evaluation ---\n",
    "    s_test_id = critic_scores(D, test_id_loader)\n",
    "    s_test_ood = critic_scores(D, test_ood_loader)\n",
    "    y_test = np.concatenate([np.ones_like(s_test_id), np.zeros_like(s_test_ood)])\n",
    "    s_test = np.concatenate([s_test_id, s_test_ood])\n",
    "\n",
    "    test_auc = auroc(y_test, s_test)\n",
    "    print(f\"Test AUROC (critic score): {test_auc:.3f}\")\n",
    "\n",
    "    # Predictions based on tau\n",
    "    y_pred_test = (s_test >= tau).astype(np.int32)\n",
    "    acc = (y_pred_test == y_test).mean()\n",
    "    print(f\"Test accuracy @tau: {acc:.3f}\")\n",
    "\n",
    "    # --- Optional: Mahalanobis on embeddings ---\n",
    "    E_val_id = critic_embeddings(D, val_id_loader)\n",
    "    E_val_ood = critic_embeddings(D, val_ood_loader)\n",
    "    ge = GaussianEmbed()\n",
    "    ge.fit(E_train_id)\n",
    "    m_val_id = -ge.maha(E_val_id)  # higher is more ID, so take negative distance\n",
    "    m_val_ood = -ge.maha(E_val_ood)\n",
    "    print(f\"Validation AUROC (Mahalanobis in critic embed): {auroc(np.concatenate([np.ones_like(m_val_id), np.zeros_like(m_val_ood)]), np.concatenate([m_val_id, m_val_ood])):.3f}\")\n",
    "\n",
    "    E_test_id = critic_embeddings(D, test_id_loader)\n",
    "    E_test_ood = critic_embeddings(D, test_ood_loader)\n",
    "    m_test_id = -ge.maha(E_test_id)\n",
    "    m_test_ood = -ge.maha(E_test_ood)\n",
    "    print(f\"Test AUROC (Mahalanobis): {auroc(np.concatenate([np.ones_like(m_test_id), np.zeros_like(m_test_ood)]), np.concatenate([m_test_id, m_test_ood])):.3f}\")\n",
    "\n",
    "    # --- Save for later inference ---\n",
    "    ckpt = {\n",
    "        \"G\": G.state_dict(),\n",
    "        \"D\": D.state_dict(),\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"tau\": tau\n",
    "    }\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    torch.save(ckpt, \"checkpoints/curve_wgan_gp.pt\")\n",
    "    print(\"Saved checkpoints/curve_wgan_gp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 9) Inference helper\n",
    "# -----------------------------------------\n",
    "class RealismScorer:\n",
    "    def __init__(self, ckpt_path: str, T=256):\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        self.mean = ckpt[\"mean\"]\n",
    "        self.std = ckpt[\"std\"]\n",
    "        self.tau = float(ckpt[\"tau\"])\n",
    "        self.T = T\n",
    "\n",
    "        self.D = Critic1D(T=T).to(device).eval()\n",
    "        self.D.load_state_dict(ckpt[\"D\"])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score(self, x_np: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        x_np: shape (T,) raw curve. Resampled already.\n",
    "        Returns: critic score (higher => more real).\n",
    "        \"\"\"\n",
    "        assert x_np.ndim == 1 and x_np.shape[0] == self.T\n",
    "        x = (x_np[None, :] - self.mean) / self.std\n",
    "        x = torch.from_numpy(x.astype(np.float32))[:, None, :]  # (1,1,T)\n",
    "        x = x.to(device)\n",
    "        s = self.D(x).item()\n",
    "        return s\n",
    "\n",
    "    def is_real(self, x_np: np.ndarray) -> bool:\n",
    "        return self.score(x_np) >= self.tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUROC (Mahalanobis in critic embed): 1.000\n",
      "Test AUROC (Mahalanobis): 1.000\n",
      "Test accuracy @tau: 0.974\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: Mahalanobis on embeddings ---\n",
    "E_train = critic_embeddings(D, train_loader)\n",
    "E_val_id = critic_embeddings(D, val_id_loader)\n",
    "E_val_ood = critic_embeddings(D, val_ood_loader)\n",
    "ge = GaussianEmbed()\n",
    "# ge.fit(E_val_id)\n",
    "ge.fit(E_train)\n",
    "m_val_id = -ge.maha(E_val_id)  # higher is more ID, so take negative distance\n",
    "m_val_ood = -ge.maha(E_val_ood)\n",
    "tau_emb = choose_threshold_from_id(m_val_id, fpr_target=0.05)\n",
    "print(f\"Validation AUROC (Mahalanobis in critic embed): {auroc(np.concatenate([np.ones_like(m_val_id), np.zeros_like(m_val_ood)]), np.concatenate([m_val_id, m_val_ood])):.3f}\")\n",
    "\n",
    "E_test_id = critic_embeddings(D, test_id_loader)\n",
    "E_test_ood = critic_embeddings(D, test_ood_loader)\n",
    "m_test_id = -ge.maha(E_test_id)\n",
    "m_test_ood = -ge.maha(E_test_ood)\n",
    "print(f\"Test AUROC (Mahalanobis): {auroc(np.concatenate([np.ones_like(m_test_id), np.zeros_like(m_test_ood)]), np.concatenate([m_test_id, m_test_ood])):.3f}\")\n",
    "\n",
    "# Predictions based on tau\n",
    "m_test = np.concatenate([m_test_id, m_test_ood])\n",
    "y_pred_test = (m_test >= tau_emb).astype(np.int32)\n",
    "acc = (y_pred_test == y_test).mean()\n",
    "print(f\"Test accuracy @tau: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUROC (Mahalanobis in critic embed): 1.000\n",
      "Test AUROC (Mahalanobis): 1.000\n",
      "Test accuracy @tau: 0.975\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: Mahalanobis on embeddings ---\n",
    "ge = GaussianEmbed()\n",
    "# ge.fit(E_val_id)\n",
    "ge.fit(X_train_id)\n",
    "m_val_id = -ge.maha(X_val_id)  # higher is more ID, so take negative distance\n",
    "m_val_ood = -ge.maha(X_val_ood)\n",
    "tau_emb = choose_threshold_from_id(m_val_id, fpr_target=0.05)\n",
    "print(f\"Validation AUROC (Mahalanobis in critic embed): {auroc(np.concatenate([np.ones_like(m_val_id), np.zeros_like(m_val_ood)]), np.concatenate([m_val_id, m_val_ood])):.3f}\")\n",
    "\n",
    "m_test_id = -ge.maha(X_test_id)\n",
    "m_test_ood = -ge.maha(X_test_ood)\n",
    "print(f\"Test AUROC (Mahalanobis): {auroc(np.concatenate([np.ones_like(m_test_id), np.zeros_like(m_test_ood)]), np.concatenate([m_test_id, m_test_ood])):.3f}\")\n",
    "\n",
    "# Predictions based on tau\n",
    "m_test = np.concatenate([m_test_id, m_test_ood])\n",
    "y_pred_test = (m_test >= tau_emb).astype(np.int32)\n",
    "acc = (y_pred_test == y_test).mean()\n",
    "print(f\"Test accuracy @tau: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\Phd\\\\Colloborator\\\\Tina Lasisi\\\\SkinSpectrum\\\\analysis'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_reflectance = pd.read_csv('../data/resample_skin_reflectance.csv').to_numpy()\n",
    "derivative = pd.read_csv('../data/resample_derivative.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_id = np.concatenate((skin_reflectance, derivative), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15255, 128)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
